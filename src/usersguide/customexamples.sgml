<chapter id="customexamples" xreflabel="Customizing the OSG Roll Examples">


<title>Customizing the OSG Roll</title>

<para>
This chapter describes some simple
customizations that can be applied to the OSG roll in Rocks with version &gt;= 5.4
</para>

<para>
A basic customization can be applied without any scripting/programming just
by setting global, appliance, or host attributes and then kickstarting a node.
</para>

<para>
Customization can be done also through rocks commands, after setting attributes.
Please see the
commands <computeroutput> rocks set attr help </computeroutput>,
<computeroutput> rocks list attr help </computeroutput>
and examples in this chapter.
</para>


<!-- ***************************************************************** -->
<section id="config-examples-condor" xreflabel="Condor Configuration Examples">
<title>Examples of Condor Configuration</title>
<para>
The following are short examples of how to customize Condor using 
Rocks commands.

<itemizedlist>

<listitem>
<para>
Enable Condor install on all VM-Containers Appliances:
<computeroutput>rocks add appliance attr vm-container OSG_Condor_Client true </computeroutput>
</para>
</listitem>

<listitem>
<para>
Disable Condor install on particular node:
<computeroutput>rocks set host attr compute-0-0 OSG_Condor_Client false </computeroutput>
</para>
</listitem>


<listitem>
<para>
Define a New Condor Master:
<computeroutput>rocks set attr OSG_Condor_Master central-master.my.edu </computeroutput>
</para>
</listitem>

<listitem>
<para>
Enable MPI/Dedicated Scheduler:
<computeroutput>rocks set attr OSG_Condor_EnableMPI true</computeroutput>. 
</para>
<para>
Actively-running Condor
daemons must be reconfigured for this attribute to take affect. 
This can be achieved dynamically on compute and frontend appliances using
<computeroutput>rocks sync host osg condor frontend compute</computeroutput>. 
</para>
<para> Reinstalled nodes
will build the correct configuration.
</para>
</listitem>

</itemizedlist>

</para>

</section> 

<!-- ***************************************************************** -->
<section id="reconfiguring-CE" xreflabel="Reconfiguring CE">
<title>Reconfiguring CE/gatekeeper</title>
<para>
A primitive configuratation of CE is done during the install, the resulting
configuration files are located in /etc/osg/config.d and is based on available 
information at install.
</para>
<note>
<para>
In case CE was not installed at kickstart, it is possible to install in a node or appliance after kickstart
by setting OSG_CE to a given scheduler (in this example "condor") on a host or appliance and then syncing as shown below
<screen>
# rocks set host attr &lt;hostname&gt; OSG_CE value=condor
# rocks sync host osg CE install &lt;hostname&gt;
</screen>
</para>
</note>
<para>
To reconfigure a CE node changes
(see table <xref linkend="customizing-ce-attrs-osg">)
on the ini files located in /etc/osg/config.d  is needed followed by running the command osg-configure -c.
This part is automatized by running the command
<screen>
# rocks sync host osg CE &lt;hostname&gt;
</screen>
</para>

<para>
This will rewrite 
10-misc.ini,
10-gateway.ini,
10-storage.ini, 
15-managedfork.ini,
20-condor.ini (or 20-sge.ini), 
30-gip.ini, 
40-network.ini,
40-siteinfo.ini
and run the osg command <computeroutput>/usr/sbin/osg-configure -c</computeroutput>
</para>

<note>
<para>
To view the script of changes for the inifiles before making changes, use
<computeroutput>rocks report host osg CE config &lt;hostname&gt;</computeroutput>
</para>
</note>

<note>
<para>
To generate the script,  login on the CE node and run
<computeroutput>rocks report host osg CE config &lt;hostname&gt; | rocks report script | bash </computeroutput>
the output script will be writen in <computeroutput>/root/CE_ini_filesConfigurator</computeroutput>
</para>
</note>

<para>
To find information about CE configuration 
please see the OSG documentation at <ulink
url="https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallComputeElement#7_Configuration_Instructions">
OSG twiki page</ulink>.
</para>

</section>

<!-- ***************************************************************** -->
<section id="reconfiguring-SE" xreflabel="Reconfiguring SE">
<title>Reconfiguring SE/bestman</title>
<para>
A initial configuration of Bestman server is done during the kickstart install, the resulting
configuration files are /etc/bestman2/conf/bestman2.rc and /etc/sysconfig/bestman2. This configuration is based on available 
information at install.
</para>
<note>
<para>
In case Bestman was not installed at kickstart, it is possible to install in a node or appliance after kickstart
by setting OSG_SE to true on a host or appliance and then syncing as shown below
<screen>
# rocks set host attr &lt;hostname&gt; OSG_SE value=true
# rocks sync host osg bestman install &lt;hostname&gt;
</screen>
</para>
</note>
<para>
To reconfigure a bestman node after changes on any attribute that build 
(see table <xref linkend="customizing-se-attrs-osg">)
those files requires rebuild the configuration by running the rocks command.
<screen>
# rocks sync host osg bestman &lt;hostname&gt;
</screen>
</para>

<para>
This will rewrite 
/etc/bestman2/conf/bestman2.rc,
/etc/sysconfig/bestman2, 
</para>

<note>
<para>
To view the script of changes for the files before making changes, use
<computeroutput>rocks report host osg bestman config &lt;hostname&gt;</computeroutput>
</para>
</note>

<note>
<para>
To generate the script,  login on the Bestman node and run
<computeroutput>rocks report host osg bestman config &lt;hostname&gt; | rocks report script | bash </computeroutput>
the output script will be writen in <computeroutput>/root/BestmanConfigurator</computeroutput>
</para>
</note>

<para>
To find information about bestman configuration 
please see the OSG documentation at <ulink
url="https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallOSGBestmanSE#4_1_Installing_BeStMan2">
OSG twiki page</ulink>.
</para>

</section>

<!-- ***************************************************************** -->
<section id="reconfiguring-WN" xreflabel="Reconfiguring WN">
<title>Reconfiguring WN OSG Client</title>
<para>
A initial configuration of Worker nodes is done during the kickstart install, the resulting
configuration dir/files are /etc/grid-security/certificates and /etc/lcmaps.db. This configuration is based on available 
information at install.
</para>
<note>
<para>
In case OSG Client was not installed at kickstart, it is possible to install in a node or appliance after kickstart
by setting OSG_Client to true on a host or appliance and then syncing as shown below
<screen>
# rocks set host attr &lt;hostname&gt; OSG_Client value=true
# rocks sync host osg wnclient install &lt;hostname&gt;
</screen>
</para>
</note>
<para>
To reconfigure a Worker/Interactive node after changes on any attribute 
that build
(see table <xref linkend="customizing-wnclient-attrs-osg">)
those files requires rebuild the configuration by running the rocks command.
<screen>
# rocks sync host osg wnclient &lt;hostname&gt;
</screen>
</para>

<para>
This will re-make link of  
/etc/grid-security/certificates, and rewrite
/etc/lcmaps.db and 
/etc/fetch-crl.d/osg-roll.conf
</para>

<note>
<para>
To view the script of changes for the files before making changes, use
<computeroutput>rocks report host osg wnclient config &lt;hostname&gt;</computeroutput>
</para>
</note>

<note>
<para>
To generate the script,  login on a Worker node and run
<computeroutput>rocks report host osg wnclient config &lt;hostname&gt; | rocks report script | bash </computeroutput>
the output script will be writen in <computeroutput>/root/wnclientConfigurator</computeroutput>
</para>
</note>

<para>
To find information about Client configuration 
please see the OSG documentation at OSG twiki page for <ulink
url="https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallWNClient">
wnclient</ulink>
and for
<ulink url="https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallGlexec">
glexec</ulink>. 
Also for the 'pilot' option in OSG_wn_LcmapsCertType see more at <ulink
url="https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/GlexecPilotCert">
this OSG twiki page</ulink>
</para>

</section>

<!-- ***************************************************************** -->
<section id="reconfiguring-xrootd" xreflabel="Reconfiguring Xrootd">
<title>Reconfiguring xrootd</title>
<para>
A initial configuration of xrootd server is done during the kickstart install, the resulting
configuration file is /etc/xrootd/xrootd-clustered.cfg. This configuration is based on available 
information at install. 
</para>
<note>
<para>
In case Xrootd was not installed at kickstart, it is possible to install in a node or appliance after kickstart
by setting OSG_XRD to true on a host or appliance and then syncing as shown below
<screen>
# rocks set host attr &lt;hostname&gt; OSG_XRD value=true
# rocks sync host osg xrootd install &lt;hostname&gt;
</screen>
</para>
</note>
<para>
To reconfigure a xrootd node after changes on any attribute 
that build 
(see table <xref linkend="customizing-xrootd-attrs-osg">)
the .cfg file requires rebuild the configuration by running the rocks command.
<screen>
# rocks sync host osg xrootd &lt;hostname&gt;
</screen>
</para>

<para>
This will rewrite 
/etc/xrootd/xrootd-clustered.cfg
</para>

<note>
<para>
To view the script of changes for the files before making changes, use
<computeroutput>rocks report host osg xrootd config &lt;hostname&gt;</computeroutput>
</para>
</note>

<note>
<para>
To generate the script,  login on the xrootd node and run
<computeroutput>rocks report host osg xrootd config &lt;hostname&gt; | rocks report script | bash </computeroutput>
the output script will be writen in <computeroutput>/root/XrootdConfigurator</computeroutput>
</para>
</note>

<para>
To find information about xrootd configuration 
please see the OSG documentation at <ulink
url="https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallXrootd#Modify_etc_xrootd_xrootd_cluster">
OSG twiki page</ulink>.
</para>

</section>

<!-- ***************************************************************** -->
<section id="reconfiguring-condor" xreflabel="Reconfiguring Condor">
<title>Reconfiguring Condor</title>
<para>
The configuration of Condor is done during the kickstart install, the resulting 
configuration files are located in /etc/condor/config.d/.
</para>
<note>
<para>
In case condor was not installed at kickstart, it is possible to install it 
in a node or appliance after kickstart by setting OSG_Condor_Client 
(or OSG_CE='condor') on a host or appliance followed by syncing as shown below:
<screen>
# rocks set host attr &lt;hostname&gt; OSG_Condor_Client value=true
# rocks sync host osg condor install &lt;hostname&gt;
</screen>
</para>
</note>
<para>
To reconfigure Condor on a node,  make appropriate attribute changes 
(see table <xref linkend="customizing-condor-attrs-osg">)
using the commands above and then run
<screen>
# rocks sync host osg condor &lt;hostname&gt;
</screen>
</para>

<para>
This will rewrite the 01_rocks_condor_config.local on the file and then calls the Condor
command <computeroutput>/usr/sbin/condor_reconfig</computeroutput>
</para>

<note>
<para>
To view the contents of the 01_rocks_condor_config.local before making changes, use
<computeroutput>rocks report host osg condor config &lt;hostname&gt;</computeroutput>
</para>
</note>


<para>
To find information about administrating  and using Condor Pools
please see the original Condor manual at <ulink
url="http://www.cs.wisc.edu/condor/manual">Condor
manuals</ulink> or <ulink url="condor-Manual"> locally </ulink>.
</para>

</section>

<!-- ***************************************************************** -->
<section id="customize-squid" xreflabel="Customize Squid">
<title>Reconfiguring Frontier Squid </title>
<para>
The default customization of Squid is done during the OS install, the resulting 
configuration file is /etc/squid/customize.sh. 
</para>
<note>
<para>
In case squid was not installed at kickstart, it is possible to install it 
in a node or appliance after kickstart by setting OSG_SQUID on a host 
or appliance followed by syncing as shown below:
<screen>
# rocks set host attr &lt;hostname&gt; OSG_SQUID value=true
# rocks sync host osg squid install &lt;hostname&gt;
</screen>
</para>
</note>
<para>
To reconfigure squid on a node, make appropriate attribute changes
(see table <xref linkend="customizing-squid-attrs-osg">)
using the commands above and then run
<screen>
# rocks sync host osg squid &lt;hostname&gt;
</screen>
</para>
<para>
This will rewrite the file /etc/squid/customize.sh then you can start the service with the
command <computeroutput>service frontier-squid start</computeroutput>
</para>

<note>
<para>
To view the contents of the /etc/squid/customize.sh before making changes, use
<computeroutput>rocks report host osg squid config &lt;hostname&gt;</computeroutput>
</para>
</note>

</section>

<!-- ***************************************************************** -->
<section id="customize-cvmfs" xreflabel="Customize CVMFS">
<title>Reconfiguring CVMFS </title>
<para>
The default customization of CVMFS is done at the OS install, the resulting configuration files are:
/etc/cvmfs/default.local, 
/etc/cvmfs/config.d/cms.cern.ch.local,
/etc/cvmfs/domain.d/cern.ch.local and
/etc/fuse.conf
</para>
<note>
<para>
In case cvmfs was not installed at kickstart, it is possible to install in a node or appliance after kickstart
by setting OSG_CVMFS on a host or appliance and then syncing as shown below
<screen>
# rocks set host attr &lt;hostname&gt; OSG_CVMFS value=true
# rocks sync host osg cvmfs install &lt;hostname&gt;
</screen>
</para>
</note>
<para>
To reconfigure cvmfs on a node, make appropriate attribute changes
(see table <xref linkend="customizing-cvmfs-attrs-osg">)
using the commands above and then run
<screen>
# rocks sync host osg cvmfs &lt;hostname&gt;
</screen>
</para>

<para>
This will rewrite the above files then you can reload the service with the
command <computeroutput>cvmfs_config reload </computeroutput>
</para>

<note>
<para>
To view the contents of the files 
/etc/cvmfs/default.local,
/etc/cvmfs/config.d/cms.cern.ch.local,
/etc/cvmfs/domain.d/cern.ch.local and 
/etc/fuse.conf
 before making changes, use
<computeroutput>rocks report host osg cvmfs config &lt;hostname&gt;</computeroutput>
</para>
</note>


</section>

<!-- ***************************************************************** -->
<section id="condor-advanced-configuration" xreflabel="Making Deeper Changes">
<title>Programatically changing the Contents of 01_rocks_condor_config.local</title>
<para>
Condor configuration is localized into /etc/condor/config.d/01_rocks_condor_config.local.
This file is generated programatically from the output of 
<computeroutput>rocks report host osg condor config &lt;hostname&gt;
</computeroutput>.
</para>
<para>
The command <computeroutput>rocks report host osg condor config</computeroutput>
is defined by the OSG roll and is written in Python. This report command is 
extensible through Rocks command plugins. 
</para>
<para>
To see a sample Condor plugin, 
view the file in location
<computeroutput>
/opt/rocks/lib/python2.4/site-packages/rocks/commands/report/host/osg/condor/config/plugin_sample.py
</computeroutput>, which is reproduced here.
<screen>
# $Id$
import rocks.commands

class Plugin(rocks.commands.Plugin):

	def provides(self):
		return 'sample'

	def run(self, argv):
		# Argv contains the hostname and the in memory key-value store
	        # that is eventually written to 
		# /etc/condor/config.d/01_rocks_condor_config.local
		# plugins can add/change/remove keys from the store

		# 1. Get the hostname and the key-value store, which
		#    is a python dictionary 
		host, kvstore = argv 

		# The following would add CONDOR_SAMPLE=Sample Plugin
		# the key = value dictionary (kvstore)  that is written out
		#
		# Example 1. Read an attribute from the database and set 
		# the values
		value = self.db.getHostAttr(host, 'Condor_HostAllow')
		kvstore['CONDOR_SAMPLE'] = value 
		
		# Example 2. Set the key CONDOR_SAMPLE to the hostname 
		kvstore['CONDOR_SAMPLE'] = host 

		# Example 3. Remove a key from the dictionary
		if 'CONDOR_SAMPLE' in kvstore:
			del kvstore['CONDOR_SAMPLE']

RollName = "condor"
</screen>
</para>

<para>
Users/Roll Developers can add their own plugins for the 
"report host condor config" command to overwrite, add, and/or delete
key,value pairs that are written into /etc/condor/config.d/01_rocks_condor_config.local.
</para>
<para>
In the above code sample, the Condor report command driver passes
the hostname and the dictionary of already defined key,value pairs
(kvstore in the sample code).  The sample code shows several different 
examples of changing the key 'CONDOR_SAMPLE'.
</para>

<para>
Plugins are written in Python, are called in random order,
 and must be named "plugin_&lt;name&gt;.py".  
</para>
<para>
Plugins also enable any 
desired configurations to be properly applied with the command
<computeroutput>
rocks sync host osg condor config
</computeroutput>.
</para>
</section>
</chapter>

