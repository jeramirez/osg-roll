<?xml version="1.0" standalone="no"?>

<kickstart >


	<description>
	The OSG Roll.
	</description>

	<copyright>
         No yet Eduardo's test	
	</copyright>

<changelog>
	$Log: osg-hadoop.xml,v $
	Revision 0.0  2012/10/05 05:48:53  eduardo
	Initial Revision
	
</changelog>

<post>

[ -f /etc/sysconfig/hadoop ]&amp;&amp; cp -p /etc/sysconfig/hadoop /etc/sysconfig/hadoop.save_original_install
<eval mode="xml">
/opt/rocks/bin/rocks report host osg hadoop config  &hostname;
</eval>
<!-- The following customization is superseded by the new rocks command
     "rocks report host osg hadoop config  &hostname;"
     This code is keep for future reference or site tweaks

sed -i -e "s#@HADOOP_NAMENODE@#&OSG_HadoopNameNode;#" /etc/sysconfig/hadoop 
sed -i -e "s#@HADOOP_REPLICATION_DEFAULT@#2#" /etc/sysconfig/hadoop
sed -i -e "s#@HADOOP_DATADIR@#&OSG_HadoopDataDir;#" /etc/sysconfig/hadoop
sed -i -e "s#=\${HADOOP_DATADIR}/data#=&OSG_HadoopData;#" /etc/sysconfig/hadoop
sed -i -e "s#@HADOOP_GANGLIA_ADDRESS@#&ganglia_address;#" /etc/sysconfig/hadoop
sed -i -e "s#@HADOOP_SECONDARY_NAMENODE@#&OSG_HadoopSecondaryNode;#" /etc/sysconfig/hadoop
sed -i -e "s#@HADOOP_CHECKPOINT_DIRS@#&OSG_HadoopCheckPointDirs;#" /etc/sysconfig/hadoop
sed -i -e "s#@HADOOP_CHECKPOINT_PERIOD@#&OSG_HadoopCheckPointPeriod;#" /etc/sysconfig/hadoop
sed -i -e "s#HADOOP_UPDATE_FSTAB=0#HADOOP_UPDATE_FSTAB=&OSG_HadoopUpdateFstab;#" /etc/sysconfig/hadoop

#  customize hadoop configuration should be for dfs.umaskmode 

sed -i -e "s#002#022#" /etc/hadoop-0.20/conf/core-site.xml.in
-->


 <!-- customize hadoop logs to only 30 days/10MB size-->
  <!-- first comment originals -->
sed -i -e "s/log4j.appender.DRFA/#ed#log4j.appender.DRFA/" /etc/hadoop-0.20/conf/log4j.properties
  <!-- second append changes -->
<file name="/etc/hadoop-0.20/conf/log4j.properties" mode="append">
#start rocks additions
#really Rolling File Appender but need on DRFA (do not know why)
log4j.appender.DRFA=org.apache.log4j.RollingFileAppender
log4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}
log4j.appender.DRFA.MaxBackupIndex=30
log4j.appender.DRFA.MaxFileSize=10MB
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
#end rocks additions
</file>

 <!-- add configuration hadoop secondary when public interface is set default but hadoop is in private network-->
<file name="/root/tweak_hadoop_secondary.sh" perms="0750" >
#!/bin/sh

if [ "&primary_net;" == "public" ]&amp;&amp;[ "&hostname;" == "&OSG_HadoopSecondaryNode;" ]; then
[ ! -f /etc/hadoop/conf/hdfs-site.xml.template_original_install ]||cp -p /etc/hadoop/conf/hdfs-site.xml.template /etc/hadoop/conf/hdfs-site.xml.template_original_install
MYIFACE=`rocks list host interface &hostname; | grep private | cut -d\  -f2`
#in case no interface info at install, use eth0 as default
if [ "$MYIFACE" == "" ]; then
MYIFACE=eth0  
fi
sed -i -e "s#&lt;/configuration&gt;#&lt;property&gt;\n  &lt;name&gt;dfs.secondary.dns.interface&lt;/name&gt;\n  &lt;value&gt;$MYIFACE&lt;/value&gt;\n  &lt;description&gt;\n    The name of the network interface from which the secondary should report its IP address.\n  &lt;/description&gt;\n&lt;/property&gt;\n&lt;/configuration&gt;#" /etc/hadoop/conf/hdfs-site.xml.template
fi
</file>

 <!-- set configuration hadoop datanode -->
/root/HadoopConfigurator

  <!-- alternate configuration hadoop daemon to use osg attributes, 
       needed when server is using public interface for hostname 
       but hadoop needs private interface to identify server 
  -->
cp -p /etc/init.d/hadoop /etc/init.d/hadoop-osg-rocks
sed -i -e "s#\`/bin/hostname -s\`#\"&hostname;\"#" /etc/init.d/hadoop-osg-rocks
sed -i -e "s#/var/lock/subsys/hadoop#/var/lock/subsys/hadoop-osg-rocks#" /etc/init.d/hadoop-osg-rocks
sed -i -e "s#'hadoop'#'hadoop-osg-rocks'#" /etc/init.d/hadoop-osg-rocks

#set directly on data nodes
#chkconfig hadoop on
#chkconfig hadoop-osg-rocks on

</post>

</kickstart> 

