<?xml version="1.0" standalone="no"?>

<kickstart>

<description>
	Configure ktune customized of grid trafic 
</description>


<changelog>
        $Log: osg-ktune-conf.xml,v $
        Revision 0.0  2014/06/30 05:48:53  eduardo
        Initial Revision
</changelog>

<!-- sl6 -->
<package cond="rocks_version_major == 6">tuned</package>
<!-- sl5 -->
<package cond="rocks_version_major == 5">ktune</package>

<main>
        <!-- kickstart 'main' commands go here -->
</main>

<pre>
        <!-- partitioning commands go here -->
</pre>


<post cond="rocks_version_major == 5">

<!-- 
  this is for SL5 
  move if needed sysctl.ktune 
-->
[ -f /etc/sysctl.ktune ]||mv /etc/sysctl.ktune /etc/sysclt.ktune.from_original_install
</post>
<post cond="(rocks_version_major == 5 or rocks_version_major == 6)" >
<!-- configure kernel for fast data transfer with ktune -->
<file name="/etc/ktune.d/osg.conf" perms="0644" >
# ktune sysctl settings for EL 5/6 servers
# taken from http://fasterdata.es.net tcp tunning + original EL5 ktune settings 

# 256 KB default performs well experimentally, and is often recommended by ISVs.
net.core.rmem_default = 262144
net.core.wmem_default = 262144

# recommended http://fasterdata.es.net
net.core.rmem_max = 16777216 
net.core.wmem_max = 16777216 

# recommended http://fasterdata.es.net
net.core.netdev_max_backlog = 30000

# recommended http://fasterdata.es.net
# increase Linux autotuning TCP buffer limits 
# min, default, and max number of bytes to use
# (only change the 3rd value, and make it 16 MB or more)
net.ipv4.tcp_rmem = 8192 87380 16777216
net.ipv4.tcp_wmem = 8192 65536 16777216


# Always have enough memory available on a UDP socket for an 8k NFS request,
# plus overhead, to prevent NFS stalling under memory pressure.  16k is still
# low enough that memory fragmentation is unlikely to cause problems.
net.ipv4.udp_rmem_min = 16384
net.ipv4.udp_wmem_min = 16384

# Ensure there's enough memory to actually allocate those massive buffers to a
# socket.
net.ipv4.tcp_mem = 8388608 12582912 16777216
net.ipv4.udp_mem = 8388608 12582912 16777216

# Filesystem I/O is usually much more efficient than swapping, so try to keep
# swapping low.  It's usually safe to go even lower than this on systems with
# server-grade storage.
vm.swappiness = 30

# If a workload mostly uses anonymous memory and it hits this limit, the entire
# working set is buffered for I/O, and any more write buffering would require
# swapping, so it's time to throttle writes until I/O can catch up.  Workloads
# that mostly use file mappings may be able to use even higher values.
vm.dirty_ratio = 50

# Ensure there's always some easily-dropped pagecache if the system is under
# memory pressure from cached files, since it's much faster to page back in than
# swap.
#vm.pagecache = 90

#from fasterdata.es.net
#requires /sbin/modprobe tcp_cubic
net.ipv4.tcp_congestion_control=htcp
</file>

chkconfig ktune on

</post>
<post cond="rocks_version_major == 7" >
<!-- configure kernel for fast data transfer  -->
<file name="/etc/sysctl.d/osg.conf" perms="0644" >
#taken from http://fasterdata.es.net/host-tuning/linux/
#For a host with a 10G NIC, optimized for network paths up to 100ms RTT, and for friendlyness to single and parallel stream tools
# allow testing with buffers up to 64MB 
net.core.rmem_max = 67108864 
net.core.wmem_max = 67108864 
# increase Linux autotuning TCP buffer limit to 32MB
net.ipv4.tcp_rmem = 4096 87380 33554432
net.ipv4.tcp_wmem = 4096 65536 33554432
# recommended default congestion control is htcp 
net.ipv4.tcp_congestion_control=htcp
# recommended for hosts with jumbo frames enabled
net.ipv4.tcp_mtu_probing=1
# recommended for CentOS7+/Debian8+ hosts
net.core.default_qdisc = fq
</file>
</post>

</kickstart> 


